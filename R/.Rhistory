install.packages("RhpcBLASctl")
#' List necessary packages here
packages <- c("splines","data.table","parallel","SuperLearner","caret",
"doParallel","doRNG","tidyverse","VIM","here","pROC")
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package, repos='http://lib.stat.cmu.edu/R/CRAN',dependencies=TRUE)
}
}
for (package in packages) {
library(package, character.only=T)
}
#' Format graphs in ggplot
thm <- theme_classic() +
theme(
legend.position = "top",
legend.title=element_blank(),
legend.background = element_rect(fill = "transparent", colour = NA),
legend.key = element_rect(fill = "transparent", colour = NA)
)
theme_set(thm)
#' Read in data and explore
a <- read_csv(here("data","analytic_data.csv")) %>% mutate(dmomrace = as.factor(dmomrace),
dmomedu2 = as.factor(dmomedu2),
mhxpara = if_else(mhxpara>10,10,mhxpara),
mhxpara = as.factor(mhxpara),
dmommari = as.factor(dmommari))
glimpse(a)
a %>% count(mhxpara)
#' Create hold-out sample for honest validation using caret package function
train <- createDataPartition(a$ptb32sp, p=0.8, list=FALSE)
training <- a[train, ]
testing <- a[-train,]
training
testing
#' Check if training and testing add up to total
(nrow(training) + nrow(testing) == nrow(a))
#' Modeling
#' Using glm logit to predict
mod_fit <- glm(ptb32sp ~ .,data=training,family=binomial(logit))
summary(mod_fit)
testing$logit_predict <- predict(mod_fit,newdata=testing,type="response")
##' NEED TO PICK ALPHA APPROPRIATELY
testing$logit_class <- as.numeric(testing$logit_predict > runif(nrow(testing)))
mean(as.numeric(testing$logit_predict))
a<-roc(testing$ptb32sp, testing$logit_predict, direction="auto")
a$auc
B<-data.frame(sens=a$sensitivities,spec=a$specificities)
head(B)
## SUPERLEARNER
#dsamp <- downSample(x,as.factor(y))
#y <- as.numeric(dsamp$Class) - 1
y <- testing$ptb32sp
x <- subset(testing,select=c(-ptb32sp,-logit_predict,-logit_class))
xgboost_learner = create.Learner("SL.xgboost",tune=list(minobspernode = 50,max_depth = c(4),shrinkage=c(.01,.1),ntrees = c(200)),env=globalenv())
glmnet_learner = create.Learner("SL.glmnet",tune=list(alpha = seq(0,1,.5)),env=globalenv())
ranger_learner = create.Learner("SL.ranger",tune=list(num.trees=c(200),mtry=c(2,3),min.node.size=c(50)),env=globalenv())
sl.lib <- c(ranger_learner$names,
xgboost_learner$names,
glmnet_learner$names,
"SL.rpartPrune",
"SL.mean",
"SL.glm",
"SL.glm.interaction",
"SL.bayesglm")
num_cores = RhpcBLASctl::get_num_cores() - 1
options(mc.cores = num_cores)
set.seed(123, "L'Ecuyer-CMRG")
fitY<-CV.SuperLearner(Y=y,X=x,family="binomial",
method="method.AUC",
SL.library=sl.lib,
cvControl=list(V=3,stratifyCV=T),
parallel = "multicore")
